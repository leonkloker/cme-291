{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.engine.input_layer import InputLayer\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/mcb-7/ibmq_belem/Pauli_Stochastic/\"\n",
    "x_path = data_path + \"circuits/all_circuits_11.npz\"\n",
    "y_path = data_path + \"indices/all_circuit_indices_11.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.load(x_path)\n",
    "x_train = x_data[\"train\"]\n",
    "x_val = x_data[\"validate\"]\n",
    "x_test = x_data[\"test\"]\n",
    "\n",
    "y_data = np.load(y_path)\n",
    "y_train = y_data[\"train\"]\n",
    "y_val = y_data[\"validate\"]\n",
    "y_test = y_data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st dimension: Index for training samples\n",
    "# 2nd dimension: Index for Qubits (5 for ibm Belem)\n",
    "# 3rd dimension: Index for moments of quantum circuit (zero padding to maximum)\n",
    "# 4th dimension: One-hot encoding of different quantum gates (+ additional infos)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(numpy_encoding, qubit_adjacency=[[0,1], [1,2], [1,3], [3,4]]):\n",
    "    \"\"\" Takes the 3-tensor numpy encoding of one quantum circuit and the architecture of a quantum computer\n",
    "    and calculates the 2-tensor encoding of shape (nqubits*nmoments, dim_gate_vector), -1 padded adjacency array\n",
    "    and global features of the circuit. \n",
    "    \"\"\"\n",
    "\n",
    "    # compute number of moments of quantum circuit\n",
    "    nmoments = np.where(np.sum((numpy_encoding != 0), axis=(0,2)) == 0)[0]\n",
    "    if len(nmoments) == 0:\n",
    "        nmoments = numpy_encoding.shape[1]\n",
    "    else:\n",
    "        nmoments = nmoments[0]\n",
    "\n",
    "    # number of qubits\n",
    "    nqubits = numpy_encoding.shape[0]\n",
    "\n",
    "    # dimensionality of the gate vector\n",
    "    dim_node_state = numpy_encoding.shape[2]\n",
    "\n",
    "    # compute adjacency list for given architecture and amount of moments\n",
    "    architecture = []\n",
    "    for i in range(nqubits):\n",
    "        architecture.append([])\n",
    "    for edge in qubit_adjacency:\n",
    "        architecture[edge[0]].append(edge[1])\n",
    "        architecture[edge[1]].append(edge[0])\n",
    "    adjacency = deepcopy(architecture)\n",
    "    for l in range(1, nmoments):\n",
    "        for i in range(0, nqubits):\n",
    "            adjacency[-1-i].append(len(adjacency)-1-i+nqubits)\n",
    "            \n",
    "        new_layer = [[n+l*nqubits for n in node] for node in architecture]\n",
    "        for i in range(0, nqubits):\n",
    "            new_layer[-1-i].append(len(adjacency)-1-i)\n",
    "\n",
    "        adjacency = adjacency + new_layer\n",
    "\n",
    "    adjacency = tf.ragged.constant(adjacency, dtype=tf.int32)\n",
    "    \n",
    "    # compute data 2-tensor with shape (nqubits*nmoments, dim_node_state)\n",
    "    data = tf.constant(np.transpose(numpy_encoding[:,:nmoments,:], axes=(1,0,2)).reshape(nqubits*nmoments, dim_node_state), dtype=tf.float32)\n",
    "\n",
    "    # compute global features of quantum circuit\n",
    "    nZ = np.sum(data[:,0] != 0)\n",
    "    nX = np.sum(data[:,1] != 0)\n",
    "    nSX = np.sum(data[:,2] != 0)\n",
    "    nCNOT = np.sum(np.any(data[:,3:7] != 0, axis=-1))\n",
    "    global_features = tf.constant([nmoments, nqubits, nZ, nX, nSX, nCNOT], shape=[6], dtype=tf.int32)\n",
    "\n",
    "    return data, adjacency, global_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator():\n",
    "    for (circuit, pst) in zip(x_train, y_train):\n",
    "        data, adjacency, global_features = transform_data(circuit)\n",
    "        yield data, adjacency, global_features, pst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 200\n",
    "BATCH_SIZE = 1\n",
    "DIM_GATE_VECTOR = 10\n",
    "GLOBAL_FEATURES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(train_data_generator, output_signature=(\n",
    "         tf.TensorSpec(shape=(None, DIM_GATE_VECTOR), dtype=tf.float32),\n",
    "         tf.RaggedTensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(GLOBAL_FEATURES), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.float64))).batch(BATCH_SIZE)\n",
    "train_dataset = train_ds.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in train_ds.take(2):\n",
    "    print(elem[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers=1, dim_node_state=10):\n",
    "    super(GraphTransformer, self).__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.dim_node_state = dim_node_state\n",
    "\n",
    "  def build(self, input):\n",
    "    self.Q = []\n",
    "    self.K = []\n",
    "    self.V = []\n",
    "    self.Skip = []\n",
    "    self.Skip_bias = []\n",
    "    for l in range(self.num_layers):\n",
    "      self.Q.append(self.add_weight(\"Query{}\".format(l), \n",
    "                                    shape=[self.dim_node_state, self.dim_node_state],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "      self.K.append(self.add_weight(\"Key{}\".format(l), \n",
    "                                    shape=[self.dim_node_state, self.dim_node_state],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "      self.V.append(self.add_weight(\"Value{}\".format(l), \n",
    "                                    shape=[self.dim_node_state, self.dim_node_state],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "      self.Skip.append(self.add_weight(\"Skip{}\".format(l),\n",
    "                                    shape=[self.dim_node_state, self.dim_node_state],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "      self.Skip_bias.append(self.add_weight(\"Skip_bias{}\".format(l),\n",
    "                                    shape=[self.dim_node_state, 1],\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "\n",
    "    self.global_features = [tf.keras.layers.Dense(self.dim_node_state, activation=\"relu\", use_bias=True)]\n",
    "    self.global_features.append(tf.keras.layers.Dense(3, activation=\"relu\", use_bias=True))\n",
    "\n",
    "    self.regression = [tf.keras.layers.Dense(128, activation=\"relu\", use_bias=True)]\n",
    "    self.regression.append(tf.keras.layers.Dense(128, activation=\"relu\", use_bias=True))\n",
    "    self.regression.append(tf.keras.layers.Dense(1, activation=\"sigmoid\", use_bias=True))\n",
    "\n",
    "# TODO implement call using tf.while_loop\n",
    "  def call(self, inputs):\n",
    "    D = []\n",
    "    for graph in inputs[1][:]:\n",
    "      n_neighbours = []\n",
    "      for n in graph:\n",
    "        n_neighbours.append(1/np.sqrt(len(n)))\n",
    "      D.append(n_neighbours)\n",
    "    \n",
    "    U = []\n",
    "    for graph in inputs[0][:]:\n",
    "      U.append(tf.transpose(graph))\n",
    "\n",
    "    graph_vector = []\n",
    "    for i in range(len(U)):\n",
    "      for l in range(self.num_layers):\n",
    "        Q = self.Q[l] @ U[i]\n",
    "        K = self.K[l] @ U[i]\n",
    "        V = self.V[l] @ U[i]\n",
    "\n",
    "        H = []\n",
    "        for n in range(Q.shape[1]):\n",
    "          neighbours = tf.constant(inputs[1][i][n], dtype=tf.int32)\n",
    "          q = tf.transpose(tf.gather(Q, indices=[n,], axis=1))\n",
    "          k = tf.gather(K, indices=neighbours, axis=1)\n",
    "          a = tf.nn.softmax(tf.math.multiply(q @ k, D[i][n]), axis=-1)\n",
    "          v = tf.gather(V, indices=neighbours, axis=1)\n",
    "          H.append(v @ tf.transpose(a))\n",
    "        H = tf.concat(H, axis=1)\n",
    "\n",
    "        #A = tf.nn.softmax(D[i] @ tf.transpose(Q) @ K, axis=-1)\n",
    "        #H = V @ tf.transpose(A)\n",
    "        S = self.Skip[l] @ U[i] + self.Skip_bias[l]\n",
    "        U[i] = tf.keras.layers.LayerNormalization(axis=1)(tf.add(S, H))\n",
    "      graph_vector.append(tf.math.reduce_mean(U[i], axis=-1))\n",
    "\n",
    "    global_features = inputs[2]\n",
    "    for l in range(len(self.global_features)):\n",
    "      global_features = self.global_features[l](global_features)\n",
    "\n",
    "    x = tf.concat([graph_vector, global_features], axis=-1)\n",
    "\n",
    "    for l in range(len(self.regression)):\n",
    "      x = self.regression[l](x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphTransformer()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch_train)\n",
    "            \n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train[3], y_pred)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XPLORE",
   "language": "python",
   "name": "xplore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60a5397fe5451e64d14e93fd677a2f02374b8d13c98c13b8d5c7089a422e322e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
