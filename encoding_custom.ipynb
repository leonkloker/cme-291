{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 03:11:06.394351: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 03:11:06.555816: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-22 03:11:07.336424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 03:11:07.336499: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 03:11:07.336505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/leonkl/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.engine.input_layer import InputLayer\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/mcb-7/ibmq_belem/Pauli_Stochastic/\"\n",
    "x_path = data_path + \"circuits/all_circuits_11.npz\"\n",
    "y_path = data_path + \"indices/all_circuit_indices_11.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.load(x_path)\n",
    "x_train = x_data[\"train\"]\n",
    "x_val = x_data[\"validate\"]\n",
    "x_test = x_data[\"test\"]\n",
    "\n",
    "y_data = np.load(y_path)\n",
    "y_train = y_data[\"train\"]\n",
    "y_val = y_data[\"validate\"]\n",
    "y_test = y_data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11620, 5, 1825, 10)\n",
      "(11620,)\n"
     ]
    }
   ],
   "source": [
    "# 1st dimension: Index for training samples\n",
    "# 2nd dimension: Index for Qubits (5 for ibm Belem)\n",
    "# 3rd dimension: Index for moments of quantum circuit (zero padding to maximum)\n",
    "# 4th dimension: One-hot encoding of different quantum gates (+ additional infos)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(numpy_encoding, qubit_adjacency=[[0,1], [1,2], [1,3], [3,4]]):\n",
    "    \"\"\" Takes the 3-tensor numpy encoding of one quantum circuit and the architecture of a quantum computer\n",
    "    and calculates the 2-tensor encoding of shape (nqubits*nmoments, dim_gate_vector), -1 padded adjacency array\n",
    "    and global features of the circuit. \n",
    "    \"\"\"\n",
    "\n",
    "    # compute number of moments of quantum circuit\n",
    "    nmoments = np.where(np.sum((numpy_encoding != 0), axis=(0,2)) == 0)[0]\n",
    "    if len(nmoments) == 0:\n",
    "        nmoments = numpy_encoding.shape[1]\n",
    "    else:\n",
    "        nmoments = nmoments[0]\n",
    "\n",
    "    # number of qubits\n",
    "    nqubits = numpy_encoding.shape[0]\n",
    "\n",
    "    # dimensionality of the gate vector\n",
    "    dim_node_state = numpy_encoding.shape[2]\n",
    "\n",
    "    # compute adjacency list for given architecture and amount of moments\n",
    "    architecture = []\n",
    "    for i in range(nqubits):\n",
    "        architecture.append([])\n",
    "    for edge in qubit_adjacency:\n",
    "        architecture[edge[0]].append(edge[1])\n",
    "        architecture[edge[1]].append(edge[0])\n",
    "    adjacency = deepcopy(architecture)\n",
    "    for l in range(1, nmoments):\n",
    "        for i in range(0, nqubits):\n",
    "            adjacency[-1-i].append(len(adjacency)-1-i+nqubits)\n",
    "            \n",
    "        new_layer = [[n+l*nqubits for n in node] for node in architecture]\n",
    "        for i in range(0, nqubits):\n",
    "            new_layer[-1-i].append(len(adjacency)-1-i)\n",
    "\n",
    "        adjacency = adjacency + new_layer\n",
    "\n",
    "    adjacency = tf.ragged.constant(adjacency, dtype=tf.int32)\n",
    "    \n",
    "    # compute data 2-tensor with shape (nqubits*nmoments, dim_node_state)\n",
    "    data = tf.constant(np.transpose(numpy_encoding[:,:nmoments,:], axes=(1,0,2)).reshape(nqubits*nmoments, dim_node_state), dtype=tf.float32)\n",
    "\n",
    "    # compute global features of quantum circuit\n",
    "    nZ = np.sum(data[:,0] != 0)\n",
    "    nX = np.sum(data[:,1] != 0)\n",
    "    nSX = np.sum(data[:,2] != 0)\n",
    "    nCNOT = np.sum(np.any(data[:,3:7] != 0, axis=-1))\n",
    "    global_features = tf.constant([nmoments, nqubits, nZ, nX, nSX, nCNOT], shape=[6], dtype=tf.int32)\n",
    "\n",
    "    return data, adjacency, global_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator():\n",
    "    for (circuit, pst) in zip(x_train, y_train):\n",
    "        data, adjacency, global_features = transform_data(circuit)\n",
    "        yield data, adjacency, global_features, pst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 200\n",
    "BATCH_SIZE = 1\n",
    "DIM_GATE_VECTOR = 10\n",
    "GLOBAL_FEATURES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 22:26:25.778161: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-21 22:26:25.778195: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-21 22:26:25.778222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (icme-gpu1): /proc/driver/nvidia/version does not exist\n",
      "2023-02-21 22:26:25.778682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(train_data_generator, output_signature=(\n",
    "         tf.TensorSpec(shape=(None, DIM_GATE_VECTOR), dtype=tf.float32),\n",
    "         tf.RaggedTensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(GLOBAL_FEATURES), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.float64))).batch(BATCH_SIZE)\n",
    "train_dataset = train_ds.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1403.], shape=(1,), dtype=float64)\n",
      "tf.Tensor([5132.], shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for elem in train_ds.take(2):\n",
    "    print(elem[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers=1, dim_node_state=10):\n",
    "    super(GraphTransformer, self).__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.dim_node_state = dim_node_state\n",
    "\n",
    "  def build(self, input):\n",
    "    self.Q = []\n",
    "    self.K = []\n",
    "    self.V = []\n",
    "    self.Skip = []\n",
    "    self.Skip_bias = []\n",
    "    for l in range(self.num_layers):\n",
    "      self.Q.append(self.add_weight(\"Query{}\".format(l), \n",
    "                                    shape=[self.dim_node_state, self.dim_node_state],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "      self.K.append(self.add_weight(\"Key{}\".format(l), \n",
    "                                    shape=[self.dim_node_state, self.dim_node_state],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "      self.V.append(self.add_weight(\"Value{}\".format(l), \n",
    "                                    shape=[self.dim_node_state, self.dim_node_state],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "      self.Skip.append(self.add_weight(\"Skip{}\".format(l),\n",
    "                                    shape=[self.dim_node_state, self.dim_node_state],\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "      self.Skip_bias.append(self.add_weight(\"Skip_bias{}\".format(l),\n",
    "                                    shape=[self.dim_node_state, 1],\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True,\n",
    "                                    dtype=tf.float32))\n",
    "\n",
    "    self.global_features = [tf.keras.layers.Dense(self.dim_node_state, activation=\"relu\", use_bias=True)]\n",
    "    self.global_features.append(tf.keras.layers.Dense(3, activation=\"relu\", use_bias=True))\n",
    "\n",
    "    self.regression = [tf.keras.layers.Dense(128, activation=\"relu\", use_bias=True)]\n",
    "    self.regression.append(tf.keras.layers.Dense(128, activation=\"relu\", use_bias=True))\n",
    "    self.regression.append(tf.keras.layers.Dense(1, activation=\"sigmoid\", use_bias=True))\n",
    "\n",
    "# TODO implement call using tf.while_loop\n",
    "  def call(self, inputs):\n",
    "    D = []\n",
    "    for graph in inputs[1][:]:\n",
    "      n_neighbours = []\n",
    "      for n in graph:\n",
    "        n_neighbours.append(1/np.sqrt(len(n)))\n",
    "      D.append(n_neighbours)\n",
    "    \n",
    "    U = []\n",
    "    for graph in inputs[0][:]:\n",
    "      U.append(tf.transpose(graph))\n",
    "\n",
    "    graph_vector = []\n",
    "    for i in range(len(U)):\n",
    "      for l in range(self.num_layers):\n",
    "        Q = self.Q[l] @ U[i]\n",
    "        K = self.K[l] @ U[i]\n",
    "        V = self.V[l] @ U[i]\n",
    "\n",
    "        H = []\n",
    "        for n in range(Q.shape[1]):\n",
    "          neighbours = tf.constant(inputs[1][i][n], dtype=tf.int32)\n",
    "          q = tf.transpose(tf.gather(Q, indices=[n,], axis=1))\n",
    "          k = tf.gather(K, indices=neighbours, axis=1)\n",
    "          a = tf.nn.softmax(tf.math.multiply(q @ k, D[i][n]), axis=-1)\n",
    "          v = tf.gather(V, indices=neighbours, axis=1)\n",
    "          H.append(v @ tf.transpose(a))\n",
    "        H = tf.concat(H, axis=1)\n",
    "\n",
    "        #A = tf.nn.softmax(D[i] @ tf.transpose(Q) @ K, axis=-1)\n",
    "        #H = V @ tf.transpose(A)\n",
    "        S = self.Skip[l] @ U[i] + self.Skip_bias[l]\n",
    "        U[i] = tf.keras.layers.LayerNormalization(axis=1)(tf.add(S, H))\n",
    "      graph_vector.append(tf.math.reduce_mean(U[i], axis=-1))\n",
    "\n",
    "    global_features = inputs[2]\n",
    "    for l in range(len(self.global_features)):\n",
    "      global_features = self.global_features[l](global_features)\n",
    "\n",
    "    x = tf.concat([graph_vector, global_features], axis=-1)\n",
    "\n",
    "    for l in range(len(self.regression)):\n",
    "      x = self.regression[l](x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = 40142828.0000\n",
      "step 1: mean loss = 131160408.0000\n",
      "step 2: mean loss = 142940352.0000\n",
      "step 3: mean loss = 158683824.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39m# Compute reconstruction loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     loss \u001b[39m=\u001b[39m mse_loss_fn(x_batch_train[\u001b[39m3\u001b[39m], y_pred)\n\u001b[0;32m---> 22\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, model\u001b[39m.\u001b[39;49mtrainable_weights)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, model\u001b[39m.\u001b[39mtrainable_weights))\n\u001b[1;32m     25\u001b[0m loss_metric(loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1112\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1106\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1107\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1108\u001b[0m           output_gradients))\n\u001b[1;32m   1109\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1112\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1114\u001b[0m     flat_targets,\n\u001b[1;32m   1115\u001b[0m     flat_sources,\n\u001b[1;32m   1116\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1117\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1118\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1121\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/ops/array_grad.py:693\u001b[0m, in \u001b[0;36m_GatherV2Grad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    688\u001b[0m   params_grad \u001b[39m=\u001b[39m indexed_slices_lib\u001b[39m.\u001b[39mIndexedSlices(values, indices,\n\u001b[1;32m    689\u001b[0m                                                  params_shape)\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m   \u001b[39m# Handle axis by transposing the axis dimension to be the first non-batch\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39m# dimension, compute the gradient and transpose the result back.\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m   outer_shape \u001b[39m=\u001b[39m params_shape[:axis]\n\u001b[1;32m    694\u001b[0m   inner_shape \u001b[39m=\u001b[39m params_shape[axis:][\u001b[39m1\u001b[39m:]\n\u001b[1;32m    695\u001b[0m   values_shape \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mconcat([outer_shape, [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], inner_shape], \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1071\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\n\u001b[1;32m   1067\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1068\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstrided_slice\u001b[39m\u001b[39m\"\u001b[39m, [tensor] \u001b[39m+\u001b[39m begin \u001b[39m+\u001b[39m end \u001b[39m+\u001b[39m strides,\n\u001b[1;32m   1069\u001b[0m     skip_on_eager\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m name:\n\u001b[1;32m   1070\u001b[0m   \u001b[39mif\u001b[39;00m begin:\n\u001b[0;32m-> 1071\u001b[0m     packed_begin, packed_end, packed_strides \u001b[39m=\u001b[39m (stack(begin), stack(end),\n\u001b[1;32m   1072\u001b[0m                                                 stack(strides))\n\u001b[1;32m   1073\u001b[0m     \u001b[39m# TODO(mdan): Instead of implicitly casting, it's better to enforce the\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m     \u001b[39m# same dtypes.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[39mif\u001b[39;00m (packed_begin\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64 \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m         packed_end\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64 \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m         packed_strides\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64):\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1466\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1464\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m     \u001b[39m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[0;32m-> 1466\u001b[0m     \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(values, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1467\u001b[0m   \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m   1468\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Input list contains non-constant tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1627\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1629\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1633\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1638\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1639\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1589\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1587\u001b[0m \u001b[39melif\u001b[39;00m dtype \u001b[39m!=\u001b[39m inferred_dtype:\n\u001b[1;32m   1588\u001b[0m   v \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(_cast_nested_seqs_to_dtype(dtype), v)\n\u001b[0;32m-> 1589\u001b[0m \u001b[39mreturn\u001b[39;00m _autopacking_helper(v, dtype, name \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mpacked\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1496\u001b[0m, in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   1493\u001b[0m   \u001b[39m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m   \u001b[39m# checking.\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, core\u001b[39m.\u001b[39mTensor) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m list_or_tuple):\n\u001b[0;32m-> 1496\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mpack(list_or_tuple, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1497\u001b[0m must_pack \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m converted_elems \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/XPLORE/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:6549\u001b[0m, in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6547\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   6548\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 6549\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   6550\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mPack\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, values, \u001b[39m\"\u001b[39;49m\u001b[39maxis\u001b[39;49m\u001b[39m\"\u001b[39;49m, axis)\n\u001b[1;32m   6551\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6552\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GraphTransformer()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch_train)\n",
    "            \n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train[3], y_pred)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XPLORE",
   "language": "python",
   "name": "xplore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60a5397fe5451e64d14e93fd677a2f02374b8d13c98c13b8d5c7089a422e322e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
